{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_pose = mp.solutions.pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from itertools import count\n",
    "from matplotlib.animation import FuncAnimation\n",
    "import time\n",
    "import psutil\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from itertools import count\n",
    "from matplotlib.animation import FuncAnimation\n",
    "plt.style.use('fivethirtyeight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataf = pd.read_csv('FinalData.csv')\n",
    "dataf = pd.read_csv('FinalData.csv', header=None)\n",
    "\n",
    "# Convert dataframe to NumPy array\n",
    "arr = dataf.to_numpy()\n",
    "\n",
    "# Transpose the array so that rows become columns\n",
    "# arr = np.transpose(arr)\n",
    "\n",
    "# Create a list of arrays, where each array represents a row\n",
    "rows = []\n",
    "for i in range(arr.shape[0]):\n",
    "    row = arr[i]\n",
    "    rows.append(row)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 2112)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(rows).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 2112)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NewTrainingArray = np.array(rows)\n",
    "(NewTrainingArray).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train = []\n",
    "No_of_Stroke_Videos = 50  \n",
    "for i in range(No_of_Stroke_Videos):\n",
    "    y_train.append(1)\n",
    "len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "No_of_No_Stroke_Videos = 50\n",
    "for i in range(No_of_No_Stroke_Videos):\n",
    "    y_train.append(0)\n",
    "len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]]\n"
     ]
    }
   ],
   "source": [
    "y_train = np.array(y_train)\n",
    "y_train=y_train[:,np.newaxis]\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trimTestArray(training_array):\n",
    "    arr_Temp = np.array(training_array, dtype=\"object\")\n",
    "    # Find the minimum number of columns across all rows\n",
    "    min_cols = np.array(rows).shape[1]\n",
    "    # Remove the last excess element from each row that has more columns than the minimum number\n",
    "    trimmed_arr = np.array([row[:min_cols] for row in arr_Temp])\n",
    "    len(trimmed_arr[0])\n",
    "    return trimmed_arr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 128)               1147392   \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 1,147,521\n",
      "Trainable params: 1,147,521\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "# Load your flattened data into X and labels into y\n",
    "X = np.array(NewTrainingArray, dtype=\"float\")\n",
    "X = X.reshape(X.shape[0], 1, X.shape[1])\n",
    "\n",
    "y = np.array(y_train, dtype=\"float\")\n",
    "\n",
    "# Define the LSTM model architecture with L2 regularization and early stopping\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(1, np.array(rows).shape[1]), kernel_regularizer=l2(0.001), recurrent_regularizer=l2(0.001)))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(1, activation='sigmoid', kernel_regularizer=l2(0.001)))\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10)\n",
    "\n",
    "# Compile the model with binary cross-entropy loss and Adam optimizer\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "# Train the model with early stopping\n",
    "history = model.fit(X, y, epochs=100, batch_size=32, validation_split=0.2, callbacks=[early_stopping])\n",
    "model.fit(X, y, epochs=1000, batch_size=32, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1D CNN MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# Load your flattened data into X and labels into y\n",
    "X = np.array(NewTrainingArray, dtype=\"float\")\n",
    "y = np.array(y_train, dtype=\"float\")\n",
    "\n",
    "# Define the CNN model architecture\n",
    "modell = Sequential()\n",
    "modell.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(np.array(rows).shape[1], 1)))\n",
    "modell.add(MaxPooling1D(pool_size=2))\n",
    "modell.add(Dropout(0.2))\n",
    "modell.add(Flatten())\n",
    "modell.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model with binary cross-entropy loss and Adam optimizer\n",
    "modell.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "modell.summary()\n",
    "# Define callbacks\n",
    "# early_stopping = EarlyStopping(monitor='val_loss', patience=10)\n",
    "checkpoint = ModelCheckpoint('best_model.h5', monitor='val_accuracy', save_best_only=True)\n",
    "\n",
    "# Fit the model with callbacks\n",
    "history = modell.fit(X, y, epochs=1000, batch_size=32, validation_split=0.3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12D CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "# Open the HDF5 file in read mode\n",
    "with h5py.File('train_file.h5', 'r') as f:\n",
    "\n",
    "    # Load the dataset into a NumPy array\n",
    "    train_array = np.array(f['my_dataset'])\n",
    "\n",
    "# Print the shape of the array to verify it was loaded correctly\n",
    "print(train_array.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80, 176, 12)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(train_array).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Dense, Flatten,Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "No_of_Stroke_Videos = 40 \n",
    "x_train = train_array\n",
    "y_train=np.concatenate((np.ones(No_of_Stroke_Videos), np.zeros(No_of_Stroke_Videos)))\n",
    "\n",
    "\n",
    "print(y_train)\n",
    "input_shape = ((train_array).shape[1], 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d (Conv1D)              (None, 174, 32)           1184      \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 87, 32)            0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 2784)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                178240    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 179,489\n",
      "Trainable params: 179,489\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "# Define the number of classes (2 for binary classification)\n",
    "num_classes = 1\n",
    "\n",
    "# Define the model architecture\n",
    "CNNmodel = Sequential()\n",
    "CNNmodel.add(Conv1D(32, kernel_size=3, activation='relu', input_shape=input_shape))\n",
    "CNNmodel.add(MaxPooling1D(pool_size=2))\n",
    "CNNmodel.add(Flatten())\n",
    "CNNmodel.add(Dense(64, activation='relu'))\n",
    "CNNmodel.add(Dropout(0.4))\n",
    "CNNmodel.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "CNNmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Define early stopping criteria\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10)\n",
    "CNNmodel.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    }
   ],
   "source": [
    "# Train the model with early stopping\n",
    "CNNmodel.fit(x_train, y_train, epochs=10, batch_size=32, validation_split=0.1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking The Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_array = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "# Final Testing \n",
    "def Test_final_CNN():\n",
    "    z = 0\n",
    "    string = \"\"\n",
    "    # TRACKING THE HANDS\n",
    "    L_shoulder_array=[]\n",
    "    L_elbow_array=[]\n",
    "    L_wrist_array=[]\n",
    "    R_shoulder_array=[]\n",
    "\n",
    "    R_elbow_array=[]\n",
    "    R_wrist_array=[]\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    ## Setup mediapipe instance\n",
    "    with mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5) as pose:\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            # Recolor image to RGB\n",
    "            image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            image.flags.writeable = False\n",
    "\n",
    "            # Make detection\n",
    "            results = pose.process(image)\n",
    "\n",
    "            # Recolor back to BGR\n",
    "            image.flags.writeable = True\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "            # Extract landmarks\n",
    "            try:\n",
    "                landmarks = results.pose_landmarks.landmark\n",
    "\n",
    "                # Get coordinates\n",
    "\n",
    "                #Left Hand\n",
    "                L_shoulder = [landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value].x,landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value].y]\n",
    "                L_elbow = [landmarks[mp_pose.PoseLandmark.LEFT_ELBOW.value].x,landmarks[mp_pose.PoseLandmark.LEFT_ELBOW.value].y]\n",
    "                L_wrist = [landmarks[mp_pose.PoseLandmark.LEFT_WRIST.value].x,landmarks[mp_pose.PoseLandmark.LEFT_WRIST.value].y]\n",
    "\n",
    "                #Right Hand\n",
    "\n",
    "                R_shoulder = [landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER.value].x,landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER.value].y]\n",
    "                R_elbow = [landmarks[mp_pose.PoseLandmark.RIGHT_ELBOW.value].x,landmarks[mp_pose.PoseLandmark.RIGHT_ELBOW.value].y]\n",
    "                R_wrist = [landmarks[mp_pose.PoseLandmark.RIGHT_WRIST.value].x,landmarks[mp_pose.PoseLandmark.RIGHT_WRIST.value].y]\n",
    "\n",
    "\n",
    "                # Putting coordinates in to arrays\n",
    "\n",
    "                #Left Hand\n",
    "\n",
    "    #             L_shoulder_array.append(landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value].x)\n",
    "    #             L_elbow_array.append(landmarks[mp_pose.PoseLandmark.LEFT_ELBOW.value].x)\n",
    "    #             L_wrist_array.append(landmarks[mp_pose.PoseLandmark.LEFT_WRIST.value].x)\n",
    "\n",
    "                #Right Hand\n",
    "\n",
    "    #             R_shoulder_array.append(landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER.value].x)\n",
    "    #             R_elbow_array.append(landmarks[mp_pose.PoseLandmark.RIGHT_ELBOW.value].x)\n",
    "    #             R_wrist_array.append(landmarks[mp_pose.PoseLandmark.RIGHT_WRIST.value].x)\n",
    "\n",
    "\n",
    "                \n",
    "                # import the time module\n",
    "                \n",
    "                # define the countdown func.\n",
    "                tabx1 = []\n",
    "                taby1 = []\n",
    "            \n",
    "                tabx2 = []\n",
    "                taby2 = []\n",
    "            \n",
    "                tabx3 = []\n",
    "                taby3 = []\n",
    "            \n",
    "                tabx4 = []\n",
    "                taby4 = []\n",
    "            \n",
    "                tabx5 = []\n",
    "                taby5 = []\n",
    "            \n",
    "                tabx6 = []\n",
    "                taby6 = []\n",
    "                testing_array = []   \n",
    "                t = 180\n",
    "                while t:\n",
    "                    \n",
    "                    tabx1.append(landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value].x)\n",
    "                    tabx2.append(landmarks[mp_pose.PoseLandmark.LEFT_ELBOW.value].x)\n",
    "                    tabx3.append(landmarks[mp_pose.PoseLandmark.LEFT_WRIST.value].x)\n",
    "                    tabx4.append(landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER.value].x)\n",
    "                    tabx5.append(landmarks[mp_pose.PoseLandmark.RIGHT_ELBOW.value].x)\n",
    "                    tabx6.append(landmarks[mp_pose.PoseLandmark.RIGHT_WRIST.value].x)\n",
    "\n",
    "                    taby1.append(landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value].y) \n",
    "                    taby2.append(landmarks[mp_pose.PoseLandmark.LEFT_ELBOW.value].y) \n",
    "                    taby3.append(landmarks[mp_pose.PoseLandmark.LEFT_WRIST.value].y) \n",
    "                    taby4.append(landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER.value].y) \n",
    "                    taby5.append(landmarks[mp_pose.PoseLandmark.RIGHT_ELBOW.value].y) \n",
    "                    taby6.append(landmarks[mp_pose.PoseLandmark.RIGHT_WRIST.value].y)\n",
    "#                     time.sleep(1)\n",
    "                    t -= 1\n",
    "                \n",
    "                if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "                    break\n",
    "                    \n",
    "                \n",
    "                df = pd.DataFrame(list(zip(tabx1,tabx2,tabx3,tabx4,tabx5,tabx6,taby1,taby2,taby3,taby4,taby5,taby6)),columns =['L_shoulder_x','L_elbow_x','L_wrist_x','R_shoulder_x','R_elbow_x','R_wrist_x','L_shoulder_y','L_elbow_y','L_wrist_y','R_shoulder_y','R_elbow_y','R_wrist_y'])\n",
    "                flattened_DataFrame = df.values.flatten()\n",
    "                testing_array.append(list(flattened_DataFrame))\n",
    "                testing_array = trimTestArray(testing_array)\n",
    "#                 print('test shape',testing_array.shape)\n",
    "                X_test = np.array(testing_array, dtype=\"float\")\n",
    "                \n",
    "                # Reshape X_test to the correct shape\n",
    "#                 X_test = X_test.reshape(X_test.shape[0], 1, X_test.shape[1])\n",
    "                \n",
    "                # Make predictions on the test data\n",
    "                y_pred = modell.predict(X_test)\n",
    "                # Convert the predicted probabilities to binary predictions\n",
    "                y_pred_binary = (y_pred > 0.5).astype(int)\n",
    "                \n",
    "                if z ==40:\n",
    "                    if y_pred_binary[0][0]==0:\n",
    "                        string = \"No Stroke\"\n",
    "                    else:\n",
    "                        string = \"Stroke\"\n",
    "                    z = 0\n",
    "                z +=1\n",
    "#                 print('z-',z)\n",
    "                \n",
    "                if string == \"No Stroke\":\n",
    "                    \n",
    "                    cv2.putText(image, string, tuple(np.multiply(0.3, [100, 48]).astype(int)), cv2.FONT_ITALIC, 0.5, (0,255,0), 2, cv2.LINE_AA)\n",
    "                else:\n",
    "                    cv2.putText(image, string, tuple(np.multiply(0.3, [100, 48]).astype(int)), cv2.FONT_ITALIC, 0.5, (0,25,255), 2, cv2.LINE_AA)\n",
    "\n",
    "#                 print(f\"Predicted label: {y_pred_binary}\")\n",
    "                \n",
    "                \n",
    "                '''\n",
    "                \n",
    "                if y_pred_binary[0][0]==1:\n",
    "                    string_temp = \"Stroke\"\n",
    "                else:\n",
    "                    string_temp = \"No Stroke\"\n",
    "                    \n",
    "                cv2.putText(image, string_temp, \n",
    "                               tuple(np.multiply(0.25, [60, 48]).astype(int)), \n",
    "                               cv2.FONT_ITALIC, 0.5, (0,25,255), 2, cv2.LINE_AA\n",
    "                                    )\n",
    "#                 print(f\"Predicted label: {y_pred_binary}\")\n",
    "                    '''\n",
    "                                \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                # Visualize coordinates on the screen\n",
    "\n",
    "#                 #Left Hand\n",
    "#                 cv2.putText(image, str(landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value].x), \n",
    "#                                tuple(np.multiply(L_shoulder, [640, 480]).astype(int)), \n",
    "#                                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255,255,255), 2, cv2.LINE_AA\n",
    "#                                     )\n",
    "#                 cv2.putText(image, str(landmarks[mp_pose.PoseLandmark.LEFT_ELBOW.value].x), \n",
    "#                                tuple(np.multiply(L_elbow, [640, 480]).astype(int)), \n",
    "#                                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255,255,255), 2, cv2.LINE_AA\n",
    "#                                     )\n",
    "#                 cv2.putText(image, str(landmarks[mp_pose.PoseLandmark.LEFT_WRIST.value].x), \n",
    "#                                tuple(np.multiply(L_wrist, [640, 480]).astype(int)), \n",
    "#                                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255,255,255), 2, cv2.LINE_AA\n",
    "#                                     )\n",
    "\n",
    "#                 #Right Hand\n",
    "\n",
    "#                 cv2.putText(image, str(landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER.value].x), \n",
    "#                                tuple(np.multiply(R_shoulder, [640, 480]).astype(int)), \n",
    "#                                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255,255,255), 2, cv2.LINE_AA\n",
    "#                                     )\n",
    "#                 cv2.putText(image, str(landmarks[mp_pose.PoseLandmark.RIGHT_ELBOW.value].x), \n",
    "#                                tuple(np.multiply(R_elbow, [640, 480]).astype(int)), \n",
    "#                                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255,255,255), 2, cv2.LINE_AA\n",
    "#                                     )\n",
    "#                 cv2.putText(image, str(landmarks[mp_pose.PoseLandmark.RIGHT_WRIST.value].x), \n",
    "#                                tuple(np.multiply(R_wrist, [640, 480]).astype(int)), \n",
    "#                                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255,255,255), 2, cv2.LINE_AA\n",
    "#                                     )\n",
    "\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "\n",
    "            # Render detections\n",
    "            mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_pose.POSE_CONNECTIONS,\n",
    "                                    mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=2), \n",
    "                                    mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2) \n",
    "                                     )               \n",
    "\n",
    "            cv2.imshow('Mediapipe Feed', image)\n",
    "\n",
    "            if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "                break\n",
    "\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "#         df = pd.DataFrame(list(zip(tabx1,tabx2,tabx3,tabx4,tabx5,tabx6,taby1,taby2,taby3,taby4,taby5,taby6)),columns =['L_shoulder_x','L_elbow_x','L_wrist_x','R_shoulder_x','R_elbow_x','R_wrist_x','L_shoulder_y','L_elbow_y','L_wrist_y','R_shoulder_y','R_elbow_y','R_wrist_y'])\n",
    "#         df\n",
    "#         flattened_DataFrame = df.values.flatten()\n",
    "        \n",
    "#         global testing_array \n",
    "#         testing_array.append(list(flattened_DataFrame))\n",
    "#         print('Success',df)\n",
    "        \n",
    "\n",
    "    # plotter(shoulder_array)\n",
    "    # plotter(elbow_array)\n",
    "    # plotter(wrist_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Test_final_CNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
